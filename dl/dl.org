#+latex_header:                {\section*{\indexname}%
#+latex_header:                 \@mkboth{\MakeUppercase\indexname}%
#+latex_header:                         {\MakeUppercase\indexname}%
#+latex_header:                 \thispagestyle{plain}\parindent\z@
#+latex_header:                 \parskip\z@ \@plus .3\p@\relax
#+latex_header:                 \columnseprule \z@
#+latex_header:                 \columnsep 35\p@
#+latex_header:                 \let\item\@idxitem}
#+latex_header:                {}
#+latex_header: \makeatother

#+latex_header: \usepackage{glossaries}
#+latex_header: \makeglossaries
#+latex_header_extra: \newglossaryentry{acronym}{name={acronym},description={An acronym is an abbreviation used as a word which is formed from the initial components in a phrase or a word. Usually these components are individual letters (as in NATO or laser) or parts of words or names (as in Benelux)}}
#+latex_header_extra: \newacronym{tla}{TLA}{Three Letter Acronym}

\maketitle
\tableofcontents

* LSTMs
** LSTM(Long Short Term Memory networks)
  One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task.

  Sometimes, we only need to look at recent information to perform the present task(e.g.: The clouds are in the _sky_). But there are also cases where we need more context(e.g.: I grew up in France... I speak fluent _French_)

  Its entirely possible for the gap between the relevant information and the point there is needed to become very large. Unfortunately as the gap grows, RNNs become unable to learn to connect the information. WHY? \\
  This is because when the net is very deep, then will be very difficult to train due to the *exploding and the vanishing gradient problems*. Both problems are caused by RNN's iterative nature, whose gradient is raised to a high power. These iterated matrix powers caused the gradient to grow or to shrink at a rate that is exponential in the numbers of time-steps.

  LSTMs are explicitly designed to avoid the long-term dependency problem.

  The key to LSTMs is the *cell state*.

  Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.
  [[./figures/pointwise multiplication operation.png]]
  An LSTM has three of these gates, *to protect and control the cell state*.


  In theory, RNNs are absolutely capable of handling such "long-term dependencies." A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult. Thankfully, LSTMs don’t have this problem! \cite{lstm1}

  #+CAPTION: Forget Gate(decide what old information to thrown away from the cell state)
  [[./figures/lstm1.png]]

  $i_t$: decides which values will update.  \\
  $\tilde{C_t}$: decides new candidate values could be added to the state.
  #+CAPTION: Input Gate(decide what new information to store in the cell state)
  [[./figures/lstm2.png]]


  #+CAPTION: Update the old state to new state.
  [[./figures/lstm3.png]]

  #+CAPTION: Union forget gate and input gate
  [[./figures/lstm3.5.png]]

  #+CAPTION: Output Gate(decide what information to output)
  [[./figures/lstm4.png]]

** Variants On LSTM1
   [[./figures/lstmV1.png]]

   [[./figures/lstmV2.png]]

** GRU(Gated Recurrent Unit)
   It combines the forget and input gates into a single "update gate"
   #+CAPTION: GRU(Gated Recurrent Unit)
   [[./figures/gru.png]]
** An Empirical Exploration of Recurrent Network Architectures
   Standard RNNs suffer from both exploding and vanishing gradients. Both problems caused by
  This is because when the net is very deep, then will be very difficult to train due to the *exploding and the vanishing gradient problems*. Both problems are caused by RNN's iterative nature, whose gradient is raised to a high power. These iterated matrix powers caused the gradient to grow or to shrink at a rate that is exponential in the numbers of time-steps.


   文章作者做了多组实验检测各种不同结构的 RNN 在不同的问题上的表现, 得到的结论包括:

    1. GRU 在除了语言模型的其他地方比 LSTM 表现好
    2. LSTM with dropout 在语言模型上表现好, 有大的遗忘门偏置后表现更好
    3. 在 LSTM 中, 各个门的重要性为: 遗忘门>输入门>输出门
    4. 遗忘门在除了语言模型外的情况下影响非常大

    语言模型的长期依赖效应强于其他场景
** LSTM Code
   [[https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/][How to Reshape Input Data for Long Short-Term Memory Networks in Keras]]
*** LSTM Input shape
    The input to every LSTM layer must be three-dimensional.

    The three dimensions of this input are:  \\
      - Samples. One sequence is one sample. A batch is comprised of one or more samples.
      - Time Steps. One time step is one point of observation in the sample.
      - Features. One feature is one observation at a time step.
*** Tips for LSTM Input
    This section lists some tips to help you when preparing your input data for LSTMs.

    - The LSTM input layer must be 3D.
    - The meaning of the 3 input dimensions are: samples, time steps, and features.
    - The LSTM input layer is defined by the input_shape argument on the first hidden layer.
    - The input_shape argument takes a tuple of two values that define the number of time steps and features.
    - The number of samples is assumed to be 1 or more.
    - The reshape() function on NumPy arrays can be used to reshape your 1D or 2D data to be 3D.
    - The reshape() function takes a tuple as an argument that defines the new shape.
* Coursera Deep Learning
  Parameters W and b (w^{l} is (n^{[l]},  n^{[l-1]}) dimensions, and b^{l} is (n^{[l]}, 1) dimensions)
** Logisstic Function
   What is the difference between the cost function and the loss function for logistic regression?
   The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.

* Transformer + Attention
* Activation function
** 性质
   1. 非线性:线性的叠加还是线性.
   2. 连续可微性: 因为使用了梯度, 所以连续可微是必要的(ReLU 虽不连续, 但同样适合做激活函数) TODO: WHY?
   3. 值域有限性: 输出优先的时候, 基于梯度下降的训练过程才能越来越稳定.
   4. 单调性: 激活函数是单调的时候, 单层的神经网络才能保证是凸函数.
   5. 具有单调导数的光滑函数: 简单有效原理
   6. 函数值和输入近似相等: 如果满足这个条件, 当权重初始化为很小的随机数时, 神经网络的训练会很高效.
** sigmoid
** Tanh
** Hard Tanh
** ReLU, Leaky ReLU, Parametric ReLU, Randomized ReLU
** ELU(Exponential Linear Unit)
** Maxout
** Softmax
** LogSoftmax
* Loss Function
** MSELoss()
** L1Loss
** BCELoss(二分类)
** BCEWithLogitsLoss(二分类)
** NLLLoss(多分类中的负对数似然损失函数)
** CrossEntropyLoss(多分类用的交叉熵损失函数)
* Optimizer
** SGD
** Momentum(加速收敛)
** AdaGrad(根据每个参数所有梯度历史平均值综合的平方根, 成反比地缩放参数, 能独立地适应调整所有模型的学习率, 实践中只在某些深度学习模型上效果不错)
** RMSProp(Hiltom 修改 AdaGrad 的梯度平方计算方式, 改变计算梯度平方累加方式为对应的指数衰减平均)
** Aam(RMSProp 与Momentum 的结合, 优点: 经过偏执校正后, 每一次迭代学习率都有一个确定的范围, 从而使得参数比较平稳)
* Regularization
** L1(会产生更稀疏的解, L1 正则化的稀疏性已广泛应用于特征选择机制)
** L2(权值衰减, 通常只针对 w, 不针对 b)
** Batch Normalization

   一种非常简便而使用的加速收敛速度技术.

   作用:
     1. 使得模型训练收敛速度更快
     2. 模型隐藏输出的特征分布更稳定, 更利于模型的学习

   在训练模型时, 可以事先将特征去相关并使它们满足一个比较好的分布
** Dropout

   按一定概率将神经网络单元暂时从网络中丢弃, 这样模型更健壮(因不会太依赖某些局部的特征)

   对于随机梯度下降来说, 由于是随机丢弃, 故而每一小批量都在训练不通的网络
* [[https://github.com/facebook/prophet][prophet]]
* References
<<bibliography link>>

bibliographystyle:unsrt
bibliography:DeepLearning.bib

#  LocalWords:  Zhenkai indexname MakeUppercase thispagestyle parindent parskip
#  LocalWords:  columnseprule columnsep makeatother usepackage makeglossaries
#  LocalWords:  newglossaryentry newacronym tla TLA maketitle tableofcontents
#  LocalWords:  png lstm et al
