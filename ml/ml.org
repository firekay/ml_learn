#+OPTIONS: ^:nil
#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup
#+TITLE: Machine Learning
#+AUTHOR: Xu Zhenkai
#+OPTIONS: toc:nil ^:{}
#+LATEX_HEADER: \usepackage{natbib}
#+LATEX_HEADER: \usepackage[version=3]{mhchem}
#+latex_header: \usepackage{makeidx}
#+latex_header: \makeindex
# This ridiculousness is to make the index start in the middle of a page.
# https://tex.stackexchange.com/questions/23870/index-shouldnt-start-new-page
#+latex_header: \makeatletter
#+latex_header: \renewenvironment{theindex}
#+latex_header:                {\section*{\indexname}%
#+latex_header:                 \@mkboth{\MakeUppercase\indexname}%
#+latex_header:                         {\MakeUppercase\indexname}%
#+latex_header:                 \thispagestyle{plain}\parindent\z@
#+latex_header:                 \parskip\z@ \@plus .3\p@\relax
#+latex_header:                 \columnseprule \z@
#+latex_header:                 \columnsep 35\p@
#+latex_header:                 \let\item\@idxitem}
#+latex_header:                {}
#+latex_header: \makeatother

#+latex_header: \usepackage{glossaries}
#+latex_header: \makeglossaries
#+latex_header_extra: \newglossaryentry{acronym}{name={acronym},description={An acronym is an abbreviation used as a word which is formed from the initial components in a phrase or a word. Usually these components are individual letters (as in NATO or laser) or parts of words or names (as in Benelux)}}
#+latex_header_extra: \newacronym{tla}{TLA}{Three Letter Acronym}

\maketitle
\tableofcontents

  [[https://en.wikipedia.org/wiki/List_of_algorithms][List of algorithms]]
* Supervised Learning
** Support Vector Machines
   In machine learning, support-vector machines (SVMs, also support-vector networks[1]) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.

   In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.

   The *support-vector clustering algorithm*, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data, and is one of the *most widely used clustering algorithms* in industrial applications.
** Linear Regression
   Like all forms of regression analysis, linear regression focuses on the *conditional probability distribution* of the response given the values of the predictors, rather than on the *joint probability distribution* of all of these variables, which is the domain of multivariate analysis.

  \begin{equation}
  \label{eq:1}
     y_{i}=\beta _{0}1+\beta _{1}x_{i1}+\cdots +\beta _{p}x_{ip}+\varepsilon _{i}=\mathbf {x} _{i}^{\mathsf {T}}{\boldsymbol {\beta }}+\varepsilon _{i},\qquad i=1,\ldots ,n,
  \end{equation}
  where $^T$ denotes the transpose, so that $x_{i}^{T} \beta$ is the inner product between vectors $x_i$ and $\beta$.

  Often these n equations are stacked together and written in matrix notation as

  \[ \mathbf {y} =X{\boldsymbol {\beta }}+{\boldsymbol {\varepsilon }}, \]

  基於均方誤差損失函數的Linear Regression有一個致命問題就會預測結果: 低偏差高方差這個是均方誤差損失函數的問題同時模型的解釋性會很差.

  L2解決了模型的準確率的問題但是可解釋性依然沒有得到解決,而且當時的計算機算力不行,特徵多了計算起來非常費時間,於是人們又開始了新算法的研究之路

  最後提出來了L1正則,其思路是儘量把一些特徵壓縮到0,這樣很顯然模型的預測biase會變大一些,但是模型的variance會降低,同時計算速度會提高很多而且最重要的是模型的解釋性能會變得很強

  最後就是L1和L2正則結合的elastic net了,這個算法克服了Lasso在一些場景的限制
    1. 當P>>N(p是特徵N是數據量)時Lasso最多隻能選N(為什麼是N需要用矩陣的知識來簡單證明一下)個個特徵這顯然不是非常合理.
　　2. 當某些特徵的相關性非常高,也就是所謂的組變量,Lasso一般傾向於只選擇其中的一個也不關心究竟要選哪一個
　　3. 對於一般N>P的情況,如果某些特徵與預測值之間的相關性很高,經驗證明預測的最終性能是Lasso占主導地位相比於Ridge
** Logistic Regression
   In statistics, the logistic model uses a logistic function to model a binary dependent variable.

   The logistic function is a sigmoid function, which takes any real input $t$, ($t \in \mathbb {R}$), and outputs a value between zero and one

   The logistic function is defined as follows:
  \begin{equation}
  \label{eq:2}
  \sigma (t)={\frac {e^{t}}{e^{t}+1}}={\frac {1}{1+e^{-t}}}
  \end{equation}

  The regression coefficients are usually estimated using maximum likelihood estimation.

  逻辑回归假设因变量 y 服从伯努利分布，而线性回归假设因变量 y 服从高斯分布。

  一个好的代价函数需要满足两个最基本的要求：能够评价模型的准确性，对参数  $\theta$ 可微。

  代价函数: 在线性回归中，最常用的是均方误差(Mean squared error), 在逻辑回归中，最常用的是代价函数是交叉熵(Cross Entropy).


\begin{equation}
\label{eq:3}
  J(\theta) = -\frac{ 1 }{ m }[\sum_{ i=1 }^{ m } ({y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log (1-h_\theta(x^{(i)})})]
\end{equation}

** Naive Bayes
** Linear Discriminant Analysis
** Decision Trees
** K-Nearest Neighbor Algorithm
** Neural Networks (Multilayer Perceptron)
** Similarity Learning
* Unsupervised Learning
** Clustering
   [[Https://En.Wikipedia.Org/Wiki/Cluster_analysis][Cluster Analysis]]
*** Hierarchical Clustering
*** K-Means
*** Mixture Models
*** Dbscan
*** Optics Algorithm
** Anomaly Detection
*** Local Outlier Factor
** Neural Networks
*** Autoencoders
*** Deep Belief Nets
*** Hebbian Learning
*** Generative Adversarial Networks
*** Self-Organizing Map
** Approaches For Learning Latent Variable Models Such As
*** Expectation–Maximization Algorithm (Em)
*** Method Of Moments
*** Blind Signal Separation Techniques
**** Principal Component Analysis
**** Independent Component Analysis
**** Non-Negative Matrix Factorization
**** Singular Value Decomposition
* Reinforcement Learning
   [[Https://En.Wikipedia.Org/Wiki/Reinforcement_learning?Action=Edit&Oldid=876586730&Wteswitched=1][Reinforcement Learning]]


* References
<<bibliography link>>

bibliographystyle:unsrt
bibliography:MachineLearning.bib
