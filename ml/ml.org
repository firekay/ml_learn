#+OPTIONS: ^:nil
#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup
#+TITLE: Machine Learning
#+AUTHOR: Xu Zhenkai
#+OPTIONS: toc:nil ^:{}
#+LATEX_HEADER: \usepackage{natbib}
#+LATEX_HEADER: \usepackage[version=3]{mhchem}
#+latex_header: \usepackage{makeidx}
#+latex_header: \makeindex
# This ridiculousness is to make the index start in the middle of a page.
# https://tex.stackexchange.com/questions/23870/index-shouldnt-start-new-page
#+latex_header: \makeatletter
#+latex_header: \renewenvironment{theindex}
#+latex_header:                {\section*{\indexname}%
#+latex_header:                 \@mkboth{\MakeUppercase\indexname}%
#+latex_header:                         {\MakeUppercase\indexname}%
#+latex_header:                 \thispagestyle{plain}\parindent\z@
#+latex_header:                 \parskip\z@ \@plus .3\p@\relax
#+latex_header:                 \columnseprule \z@
#+latex_header:                 \columnsep 35\p@
#+latex_header:                 \let\item\@idxitem}
#+latex_header:                {}
#+latex_header: \makeatother

#+latex_header: \usepackage{glossaries}
#+latex_header: \makeglossaries
#+latex_header_extra: \newglossaryentry{acronym}{name={acronym},description={An acronym is an abbreviation used as a word which is formed from the initial components in a phrase or a word. Usually these components are individual letters (as in NATO or laser) or parts of words or names (as in Benelux)}}
#+latex_header_extra: \newacronym{tla}{TLA}{Three Letter Acronym}

\maketitle
\tableofcontents

  [[https://en.wikipedia.org/wiki/List_of_algorithms][List of algorithms]]
* Introduction
** What Is Machine Learning
   Arthur Samuel(1959). The field of study that gives computers the ability to learn without being explicitly programmed.

   Tom Mitchell(1998a). A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.
** Supervised Learning
   We are giving a data set and already known what our correct output look like, having an idea that there is a relationship between input and output.

   Supervised Learning problems are categorized into "regression" and "classification".
** Unsupervised Learning
   Unsupervised learning allows us to approach problems with little or no idea what our results should look like.

   We can derive structure by clustering the data based on relationships amoung the variables in the data.
** Parameter Learning
   Gradient Descent: The way we do this is by taking the derivative (the tangential line to a function) of our cost function.
* Supervised Learning
** Support Vector Machines
   In machine learning, support-vector machines (SVMs, also support-vector networks[1]) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.

   In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.

   The *support-vector clustering algorithm*, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data, and is one of the *most widely used clustering algorithms* in industrial applications.
** Linear Regression
   Like all forms of regression analysis, linear regression focuses on the *conditional probability distribution* of the response given the values of the predictors, rather than on the *joint probability distribution* of all of these variables, which is the domain of multivariate analysis.

  \begin{equation}
  \label{eq:1}
     y_{i}=\beta _{0}1+\beta _{1}x_{i1}+\cdots +\beta _{p}x_{ip}+\varepsilon _{i}=\mathbf {x} _{i}^{\mathsf {T}}{\boldsymbol {\beta }}+\varepsilon _{i},\qquad i=1,\ldots ,n,
  \end{equation}
  where $^T$ denotes the transpose, so that $x_{i}^{T} \beta$ is the inner product between vectors $x_i$ and $\beta$.

  Often these n equations are stacked together and written in matrix notation as

  \[ \mathbf {y} =X{\boldsymbol {\beta }}+{\boldsymbol {\varepsilon }}, \]

  基於均方誤差損失函數的 Linear Regression 有一個致命問題就會預測結果: 低偏差高方差這個是均方誤差損失函數的問題同時模型的解釋性會很差.

  L2 解決了模型的準確率的問題但是可解釋性依然沒有得到解決,而且當時的計算機算力不行,特徵多了計算起來非常費時間,於是人們又開始了新算法的研究之路

  最後提出來了 L1 正則,其思路是儘量把一些特徵壓縮到 0,這樣很顯然模型的預測 biase 會變大一些,但是模型的 variance 會降低,同時計算速度會提高很多而且最重要的是模型的解釋性能會變得很強

  最後就是 L1 和 L2 正則結合的 elastic net 了,這個算法克服了 Lasso 在一些場景的限制
    1. 當 P>>N(p 是特徵 N 是數據量)時 Lasso 最多隻能選 N(為什麼是 N 需要用矩陣的知識來簡單證明一下)個個特徵這顯然不是非常合理.
　　2. 當某些特徵的相關性非常高,也就是所謂的組變量,Lasso 一般傾向於只選擇其中的一個也不關心究竟要選哪一個
　　3. 對於一般 N>P 的情況,如果某些特徵與預測值之間的相關性很高,經驗證明預測的最終性能是 Lasso 占主導地位相比於 Ridge
** Logistic Regression
   In statistics, the logistic model uses a logistic function to model a binary dependent variable.

   The logistic function is a sigmoid function, which takes any real input $t$, ($t \in \mathbb {R}$), and outputs a value between zero and one

   The logistic function is defined as follows:
  \begin{equation}
  \label{eq:2}
  \sigma (t)={\frac {e^{t}}{e^{t}+1}}={\frac {1}{1+e^{-t}}}
  \end{equation}

  The regression coefficients are usually estimated using maximum likelihood estimation.

  逻辑回归假设因变量 y 服从伯努利分布，而线性回归假设因变量 y 服从高斯分布。

  一个好的代价函数需要满足两个最基本的要求：能够评价模型的准确性，对参数  $\theta$ 可微。

  代价函数: 在线性回归中，最常用的是均方误差(Mean squared error), 在逻辑回归中，最常用的是代价函数是交叉熵(Cross Entropy).


\begin{equation}
\label{eq:3}
  J(\theta) = -\frac{ 1 }{ m }[\sum_{ i=1 }^{ m } ({y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log (1-h_\theta(x^{(i)})})]
\end{equation}

** XGBOOST
   https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
***  The XGBoost Advantage
    XGBoost (eXtreme Gradient Boosting) is an advanced implementation of gradient boosting algorithm.

    GBM(Gradient Boosting)

    1. Regularization
       XGBoost is also known as 'regularized boosting' technique.
    2. Parallel Processing
       XGBoost implements parallel processing and is blazingly faster as compared to GBM.
    3. High Flexibility
       XGBoost allow users to define custom optimization objectives and evaluation criteria.

       This adds a whole new dimension to the model and there is no limit to what we can do.s

    4. Handling Missing Values
       XGBoost has an in-built routine to handle missing values.

       User is required to supply a different value than other observations and pass that as a parameter.
    5. Tree Pruning
       A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm.

       XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.

       Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.
    6. Built-in Cross-Validation
       XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.

       This is unlike GBM where we have to run a grid-search and only a limited values can be tested.
    7. Continue on Existing Model
       User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.

       GBM implementation of sklearn also has this feature so they are even on this point.
*** XGBoost Parameters
    The overall parameters have been divided into 3 categories by XGBoost authors:
    1. General Parameters: Guide the overall functioning
    2. Booster Parameters: Guide the individual booster (tree/regression) at each step
    3. Learning Task Parameters: Guide the optimization performed

**** General Parameters
     These define the overall functionality of XGBoost.

     1. booster [default=gbtree]
       Select the type of model to run at each iteration. It has 2 options:
         - gbtree: tree-based models
         - gblinear: linear models
     2. silent [default=0]:
       - Silent mode is activated is set to 1, i.e. no running messages will be printed.
       - It’s generally good to keep it 0 as the messages might help in understanding the model.
     3. nthread [default to maximum number of threads available if not set]
       - This is used for parallel processing and number of cores in the system should be entered
       - If you wish to run on all cores, value should not be entered and algorithm will detect automatically
**** Booster Parameters
     Though there are 2 types of boosters, I’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.
     1. eta [default=0.3]
        - Analogous to learning rate in GBM
        - Makes the model more robust by shrinking the weights on each step
        - Typical final values to be used: 0.01-0.2
     2. min_child_weight [default=1]
        - Defines the minimum sum of weights of all observations required in a child.
        - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.
        - *Used to control over-fitting*. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.
        - *Too high values can lead to under-fitting hence*, _it should be tuned using CV_.
     3. max_depth [default=6]
        - The maximum depth of a tree, same as GBM.
        - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.
        - *Should be tuned using CV*.
        - Typical values: 3-10
     4. max_leaf_nodes
        - The maximum number of terminal nodes or leaves in a tree.
        - _Can be defined in place of max_depth_. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.
        - If this is defined, GBM will ignore max_depth.
     5. gamma [default=0]
        - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.
        - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.
     6. max_delta_step [default=0]
        - In maximum delta step we allow each tree’s weight estimation to be. _If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative._
        - *Usually this parameter is not needed*, but it might help in logistic regression when class is extremely imbalanced.
        - This is generally not used but you can explore further if you wish.
     7. subsample [default=1]
        - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.
        - _Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting._
        - Typical values: 0.5-1
     8. colsample_bytree [default=1]
        - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.
        - Typical values: 0.5-1
     9. colsample_bylevel [default=1]
        - Denotes the subsample ratio of columns for each split, in each level.
        - _I don’t use this often because subsample and colsample_bytree will do the job for you_. but you can explore further if you feel so.
     10. lambda [default=1]
        - L2 regularization term on weights (analogous to Ridge regression)
        - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.
     11. alpha [default=0]
        - L1 regularization term on weight (analogous to Lasso regression)
        - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented
     12. scale_pos_weight [default=1]
        A value greater than 0 should be *used in case of high class imbalance* as it *helps in faster convergence*.
**** Learning Task Parameters
     These parameters are used to define the optimization objective the metric to be calculated at each step.
     1. objective [default=reg:linear]
        This defines the loss function to be minimized.  Mostly used values are:
        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)
        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)
        - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes
        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.
     2. eval_metric [ default according to objective ]
        The metric to be used for validation data.
        The default values are rmse for regression and error for classification.
        Typical values are:
        - rmse – root mean square error
        - mae – mean absolute error
        - logloss – negative log-likelihood
        - error – Binary classification error rate (0.5 threshold)
        - merror – Multiclass classification error rate
        - mlogloss – Multiclass logloss
        - auc: Area under the curve
     3. seed [default=0]
        The random number seed.
        *Can be used for generating reproducible results and also for parameter tuning*.

*** Key Thoughts
    As we come to the end, I would like to share 2 key thoughts:
      - It is difficult to get a very big leap in performance by just using parameter tuning or slightly better models. The max score for GBM was 0.8487 while XGBoost gave 0.8494. This is a decent improvement but not something very substantial.
      - A significant jump can be obtained by other methods like feature engineering, creating ensemble of models, stacking, etc
*** [[https://plushunter.github.io/2017/01/26/%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AE%2597%25E6%25B3%2595%25E7%25B3%25BB%25E5%2588%2597%25EF%25BC%25888%25EF%25BC%2589%25EF%25BC%259AXgBoost/][机器学习算法系列（8）：XgBoost]]
    监督学习四要素: 模型(model), 参数(parameters), 目标函数(object-functions), 优化算法(optimization algorithm)
*** [[https://blog.csdn.net/sb19931201/article/details/52557382][xgboost 入门与实战（原理篇）]]
** Naive Bayes
** Linear Discriminant Analysis
** Decision Trees
** K-Nearest Neighbor Algorithm
** Neural Networks (Multilayer Perceptron)
** Similarity Learning
* Unsupervised Learning
** Clustering
   [[Https://En.Wikipedia.Org/Wiki/Cluster_analysis][Cluster Analysis]]
*** Hierarchical Clustering
*** K-Means
     k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.
*** Mixture Models
*** Dbscan
*** Optics Algorithm
** Anomaly Detection
*** Local Outlier Factor
** Neural Networks
*** Autoencoders
*** Deep Belief Nets
*** Hebbian Learning
*** Generative Adversarial Networks
*** Self-Organizing Map
** Approaches For Learning Latent Variable Models
*** Expectation–Maximization Algorithm (Em)
*** Method Of Moments
*** Blind Signal Separation Techniques
**** Principal Component Analysis
**** Independent Component Analysis
**** Non-Negative Matrix Factorization
**** Singular Value Decomposition
* Reinforcement Learning
   [[Https://En.Wikipedia.Org/Wiki/Reinforcement_learning?Action=Edit&Oldid=876586730&Wteswitched=1][Reinforcement Learning]]
* Index
** PSI(population stability index)
   In simple words, Population Stability Index (PSI) compares the distribution of a scoring variable (predicted probability) in scoring data set to a training data set that was used to develop the model. The idea is to check "How the current scoring is compared to the predicted probability from training data set".



* References
<<bibliography link>>

bibliographystyle:unsrt
bibliography:MachineLearning.bib
